{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# import matplotlib.pyplot as plt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layer_init, BetaHead, make_env\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[1;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[1;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[1;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[1;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[1;32m     17\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/parameter.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disabled_torch_function_impl\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Metaclass to combine _TensorMeta and the instance check override for Parameter.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ParameterMeta\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TensorMeta):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Make `isinstance(t, Parameter)` return True for custom tensor instances that have the _is_param flag.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs, gaussian=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_gaussian = gaussian\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 256)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(256, 256)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(256, 1), std=1.0),\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 256)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(256, 256)),\n",
    "            nn.Tanh(),\n",
    "            BetaHead(256, np.prod(envs.single_action_space.shape)),\n",
    "        )\n",
    "\n",
    "    def expand_layer(self, network, layer_idx, new_out_features):\n",
    "        \"\"\" Expands a given layer and updates the next layer's input features \"\"\"\n",
    "        old_layer = network[layer_idx]\n",
    "        next_layer = network[layer_idx + 2]  # Skip activation function (Tanh)\n",
    "\n",
    "        if not isinstance(old_layer, nn.Linear) or not isinstance(next_layer, nn.Linear):\n",
    "            raise ValueError(f\"Layers at indices {layer_idx} and {layer_idx+2} must be Linear layers.\")\n",
    "\n",
    "        in_features = old_layer.in_features\n",
    "        old_out_features = old_layer.out_features\n",
    "\n",
    "        # Expand first layer\n",
    "        new_layer = nn.Linear(in_features, new_out_features)\n",
    "        new_layer.weight.data[:old_out_features, :] = old_layer.weight.data\n",
    "        new_layer.bias.data[:old_out_features] = old_layer.bias.data\n",
    "        nn.init.xavier_uniform_(new_layer.weight.data[old_out_features:, :])\n",
    "        nn.init.zeros_(new_layer.bias.data[old_out_features:])\n",
    "\n",
    "        # Expand second layer (update input features)\n",
    "        old_next_weights = next_layer.weight.data.clone()\n",
    "        old_next_bias = next_layer.bias.data.clone()\n",
    "\n",
    "        new_next_layer = nn.Linear(new_out_features, next_layer.out_features)\n",
    "        new_next_layer.weight.data[:, :old_out_features] = old_next_weights\n",
    "        new_next_layer.bias.data = old_next_bias\n",
    "        nn.init.xavier_uniform_(new_next_layer.weight.data[:, old_out_features:])\n",
    "        \n",
    "        # Replace layers in network\n",
    "        network[layer_idx] = new_layer\n",
    "        network[layer_idx + 2] = new_next_layer  # Skip activation function\n",
    "\n",
    "    def expand_actor(self, layer_idx, new_out_features):\n",
    "        self.expand_layer(self.actor, layer_idx, new_out_features)\n",
    "\n",
    "    def expand_critic(self, layer_idx, new_out_features):\n",
    "        self.expand_layer(self.critic, layer_idx, new_out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
